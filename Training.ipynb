{"cells":[{"cell_type":"markdown","metadata":{"id":"gERKBiEqMFQk"},"source":["## Import all modules"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22485,"status":"ok","timestamp":1715880423475,"user":{"displayName":"Alexis Limozin","userId":"15899913993095348392"},"user_tz":-120},"id":"HulmjBHOPiB6","outputId":"c51afd4c-e5ac-49c0-fb10-7e13516d1b01"},"outputs":[],"source":["import os, sys\n","# from google.colab import drive\n","# drive.mount('/content/drive/')\n","root = '/Users/alexislimozin/Documents/hate-speech-recognition'"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":81990,"status":"ok","timestamp":1715880505460,"user":{"displayName":"Alexis Limozin","userId":"15899913993095348392"},"user_tz":-120},"id":"yn1wK5dYLe8X","outputId":"79c56cb8-fed8-4a1e-f1ce-8e66abaafc00"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/alexislimozin/.pyenv/versions/3.12.2/envs/HateEnv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import DebertaV2Tokenizer, DebertaV2ForSequenceClassification\n","from transformers import AutoConfig\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","from smart_pytorch import SMARTLoss, kl_loss, sym_kl_loss\n","from transformers import BertTokenizer\n"]},{"cell_type":"markdown","metadata":{"id":"hkYZ5Mm5olyP"},"source":["## Tokenization"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3151,"status":"ok","timestamp":1715882041158,"user":{"displayName":"Alexis Limozin","userId":"15899913993095348392"},"user_tz":-120},"id":"MsPAKY9WokM9"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/alexislimozin/.pyenv/versions/3.12.2/envs/HateEnv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]}],"source":["tokenizer = DebertaV2Tokenizer.from_pretrained('microsoft/deberta-v3-xsmall')"]},{"cell_type":"markdown","metadata":{"id":"dOkVD-ozMBXG"},"source":["## Import datasets"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":575,"status":"ok","timestamp":1715881164256,"user":{"displayName":"Alexis Limozin","userId":"15899913993095348392"},"user_tz":-120},"id":"cULPTLI4oDQp","outputId":"521d9385-3fda-416c-8c6a-62989f689af5"},"outputs":[],"source":["train_text_path = os.path.join(root, \"Split Data\", \"text_train.npy\")\n","train_label_path = os.path.join(root, \"Split Data\", \"label_train.npy\")\n","test_text_path = os.path.join(root, \"Split Data\", \"text_test.npy\")\n","test_label_path = os.path.join(root, \"Split Data\", \"label_test.npy\")\n","\n","# Load the data from .npy files with allow_pickle=True\n","text_train = np.load(train_text_path, allow_pickle=True)\n","label_train = np.load(train_label_path, allow_pickle=True)\n","text_test = np.load(test_text_path, allow_pickle=True)\n","label_test = np.load(test_label_path, allow_pickle=True)\n","\n","# Create a custom dataset class\n","class CustomDataset(Dataset):\n","    def __init__(self, text_data, label_data, tokenizer, max_length=512):\n","        self.text_data = text_data\n","        self.label_data = label_data\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.text_data)\n","\n","    def __getitem__(self, index):\n","      text = self.text_data[index]\n","      label = self.label_data[index]\n","\n","      # Preprocess the text using the tokenizer\n","      # Convert text to a PyTorch tensor of token ids\n","      encoded_text = self.tokenizer(\n","            text,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='pt'\n","      )\n","\n","      input_ids = encoded_text['input_ids'].squeeze(0)  # Remove the extra dimension\n","      attention_mask = encoded_text['attention_mask'].squeeze(0)\n","\n","      # Convert label to a PyTorch tensor\n","      # First, convert the label data from a string to an integer\n","      if isinstance(label, str):\n","          label = int(label)  # Convert label from string to integer\n","\n","      return {\n","            'input_ids': input_ids,\n","            'attention_mask': attention_mask,\n","            'labels': torch.tensor(label, dtype=torch.int64)\n","        }\n","\n","\n","# Create the datasets for training and testing\n","train_dataset = CustomDataset(text_train, label_train, tokenizer)\n","test_dataset = CustomDataset(text_test, label_test, tokenizer)\n","\n","# DataLoader parameters\n","batch_size = 32\n","shuffle = True\n","\n","# Create the DataLoader instances for training and testing\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle)\n","\n","# Example usage of the DataLoader\n","# for batch_idx, batch in enumerate(train_loader):\n","#     print(f'Batch {batch_idx + 1}:')\n","#     print('Text batch:', batch['input_ids'][0])\n","#     print('Attention mask batch:', batch['attention_mask'][0])\n","#     print('Label batch:', batch['labels'][0])\n","#     break  # Remove this break statement to go through the whole DataLoader\n"]},{"cell_type":"markdown","metadata":{"id":"D1ReSwbnMPaT"},"source":["## Main module"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":455,"status":"ok","timestamp":1715881931153,"user":{"displayName":"Alexis Limozin","userId":"15899913993095348392"},"user_tz":-120},"id":"b9kZ4b6PaQDA"},"outputs":[],"source":["class SMARTDeBERTaClassificationModel(nn.Module):\n","\n","    def __init__(self, model, weight = 0.02):\n","        super().__init__()\n","        self.model = model\n","        self.weight = weight\n","\n","    def forward(self, input_ids, attention_mask, labels):\n","        # Get initial embeddings\n","        embedder = self.model.get_input_embeddings()\n","        embed = embedder(input_ids)\n","        \n","        # Define eval function\n","        def eval(embed):\n","            outputs = self.model(inputs_embeds=embed, attention_mask=attention_mask, labels=labels)\n","            return outputs.logits\n","\n","        # Define SMART loss\n","        smart_loss_fn = SMARTLoss(eval_fn = eval, loss_fn = kl_loss, loss_last_fn = sym_kl_loss)\n","        # Compute initial (unperturbed) state\n","        state = eval(embed)\n","        # Apply classification loss\n","        loss = F.cross_entropy(state.view(-1, 2), labels.view(-1))\n","        # Apply smart loss\n","        loss += self.weight * smart_loss_fn(embed, state)\n","\n","        return state, loss"]},{"cell_type":"markdown","metadata":{"id":"4kenVzSzMVZI"},"source":["## Training"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":455},"executionInfo":{"elapsed":2779,"status":"error","timestamp":1715882050123,"user":{"displayName":"Alexis Limozin","userId":"15899913993095348392"},"user_tz":-120},"id":"O89onAjJgI2G","outputId":"648650ec-002f-4d5d-83f8-703552cfed83"},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-xsmall and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["device = torch.device(\"gpu\" if torch.gpu.is_available() else \"cpu\")\n","print(device)\n","# Load configuration from pre-trained\n","config = AutoConfig.from_pretrained('microsoft/deberta-v3-xsmall', num_labels=2)\n","base_model = DebertaV2ForSequenceClassification(config).from_pretrained('microsoft/deberta-v3-xsmall')\n","model = SMARTDeBERTaClassificationModel(base_model, weight=0.02).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":536,"status":"ok","timestamp":1715880790456,"user":{"displayName":"Alexis Limozin","userId":"15899913993095348392"},"user_tz":-120},"id":"dTSijC3ml58G"},"outputs":[],"source":["def train_model(model, dataloader, optimizer, device):\n","    model.train()  # Set the model to training mode\n","    total_loss = 0\n","    total_steps = len(dataloader)\n","    for batch_idx, batch in enumerate(dataloader):\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        # Zero the parameter gradients\n","        optimizer.zero_grad()\n","        \n","        # Forward pass\n","        _, loss = model(input_ids, attention_mask, labels)\n","\n","        # Backward and optimize\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","        # Print loss every few batches\n","        print(f'Batch {batch_idx + 1}/{total_steps}, Batch Loss: {loss.item():.4f}')\n","\n","\n","    avg_loss = total_loss / len(dataloader)\n","    print(f'Training Loss: {avg_loss:.4f}')\n","    return avg_loss"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1715880544739,"user":{"displayName":"Alexis Limozin","userId":"15899913993095348392"},"user_tz":-120},"id":"0iQLBjyQnVmc"},"outputs":[],"source":["def compute_metrics(pred_labels, true_labels):\n","    accuracy = accuracy_score(true_labels, pred_labels)\n","    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, pred_labels, average='binary')\n","    return {\n","        \"accuracy\": accuracy,\n","        \"precision\": precision,\n","        \"recall\": recall,\n","        \"f1\": f1\n","    }\n"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1715880544739,"user":{"displayName":"Alexis Limozin","userId":"15899913993095348392"},"user_tz":-120},"id":"_zecWOgmnjlc"},"outputs":[],"source":["def evaluate_model(model, dataloader, device):\n","    model.eval()  # Set the model to evaluation mode\n","    predictions = []\n","    true_labels = []\n","\n","    with torch.no_grad():  # Disable gradient computation\n","        for batch in dataloader:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","\n","            outputs, _ = model(input_ids, attention_mask, labels)\n","            _, preds = torch.max(outputs, dim=1)\n","\n","            predictions.extend(preds.cpu().numpy())\n","            true_labels.extend(labels.cpu().numpy())\n","\n","    # Compute the metrics\n","    metrics = compute_metrics(predictions, true_labels)\n","    return metrics"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":474,"status":"error","timestamp":1715880795707,"user":{"displayName":"Alexis Limozin","userId":"15899913993095348392"},"user_tz":-120},"id":"ZziNZ9D7mBJu","outputId":"360a4af0-d652-4a15-dce6-f8db7f504603"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/3\n","Batch 1/1066, Batch Loss: 0.6963\n","Batch 2/1066, Batch Loss: 0.6849\n","Batch 3/1066, Batch Loss: 0.6746\n","Batch 4/1066, Batch Loss: 0.6781\n","Batch 5/1066, Batch Loss: 0.6895\n","Batch 6/1066, Batch Loss: 0.6803\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m evaluate_model(model, test_loader, device)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Metrics: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n","Cell \u001b[0;32mIn[10], line 17\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, optimizer, device)\u001b[0m\n\u001b[1;32m     14\u001b[0m _, loss \u001b[38;5;241m=\u001b[39m model(input_ids, attention_mask, labels)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     20\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m~/.pyenv/versions/3.12.2/envs/HateEnv/lib/python3.12/site-packages/torch/_tensor.py:523\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    515\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    516\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    521\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    522\u001b[0m     )\n\u001b[0;32m--> 523\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.pyenv/versions/3.12.2/envs/HateEnv/lib/python3.12/site-packages/torch/autograd/__init__.py:284\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    279\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.pyenv/versions/3.12.2/envs/HateEnv/lib/python3.12/site-packages/torch/autograd/graph.py:767\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    765\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 767\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    768\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    771\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","File \u001b[0;32m~/.pyenv/versions/3.12.2/envs/HateEnv/lib/python3.12/site-packages/torch/autograd/function.py:287\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBackwardCFunction\u001b[39;00m(_C\u001b[38;5;241m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[1;32m    283\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03m    This class is used for internal autograd work. Do not use.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    288\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;124;03m        Apply method used when executing this Node during the backward\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[1;32m    291\u001b[0m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[1;32m    292\u001b[0m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["num_epochs = 3  # Define the number of epochs\n","\n","for epoch in range(num_epochs):\n","    print(f'Epoch {epoch+1}/{num_epochs}')\n","    train_model(model, train_loader, optimizer, device)\n","\n","    metrics = evaluate_model(model, test_loader, device)\n","    print(f'Validation Metrics: {metrics}')"]},{"cell_type":"markdown","metadata":{"id":"5IQaH4HgMXvI"},"source":["## Results\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oWriZQiKMb_a"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}
