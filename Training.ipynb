{"cells":[{"cell_type":"markdown","metadata":{"id":"gERKBiEqMFQk"},"source":["## Import all modules"]},{"cell_type":"markdown","metadata":{},"source":["# Training.py"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":81990,"status":"ok","timestamp":1715880505460,"user":{"displayName":"Alexis Limozin","userId":"15899913993095348392"},"user_tz":-120},"id":"yn1wK5dYLe8X","outputId":"79c56cb8-fed8-4a1e-f1ce-8e66abaafc00"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/alexislimozin/.pyenv/versions/3.10.4/envs/HateEnv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import os, sys\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torch.cuda.amp import GradScaler, autocast\n","from transformers import DebertaV2Tokenizer, DebertaV2ForSequenceClassification\n","from transformers import AutoConfig\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","from smart_pytorch import SMARTLoss, kl_loss, sym_kl_loss\n","from transformers import BertTokenizer"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["root = '/Users/alexislimozin/Documents/hate-speech-recognition'"]},{"cell_type":"markdown","metadata":{"id":"hkYZ5Mm5olyP"},"source":["## Tokenization"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3151,"status":"ok","timestamp":1715882041158,"user":{"displayName":"Alexis Limozin","userId":"15899913993095348392"},"user_tz":-120},"id":"MsPAKY9WokM9"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/alexislimozin/.pyenv/versions/3.10.4/envs/HateEnv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]}],"source":["tokenizer = DebertaV2Tokenizer.from_pretrained('microsoft/mdeberta-v3-base')"]},{"cell_type":"markdown","metadata":{"id":"dOkVD-ozMBXG"},"source":["## Import datasets"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":575,"status":"ok","timestamp":1715881164256,"user":{"displayName":"Alexis Limozin","userId":"15899913993095348392"},"user_tz":-120},"id":"cULPTLI4oDQp","outputId":"521d9385-3fda-416c-8c6a-62989f689af5"},"outputs":[],"source":["train_text_path = os.path.join(root, \"Split Data\", \"text_train.npy\")\n","train_label_path = os.path.join(root, \"Split Data\", \"label_train.npy\")\n","test_text_path = os.path.join(root, \"Split Data\", \"text_test.npy\")\n","test_label_path = os.path.join(root, \"Split Data\", \"label_test.npy\")\n","\n","# Load the data from .npy files with allow_pickle=True\n","text_train = np.load(train_text_path, allow_pickle=True)\n","label_train = np.load(train_label_path, allow_pickle=True)\n","text_test = np.load(test_text_path, allow_pickle=True)\n","label_test = np.load(test_label_path, allow_pickle=True)\n","\n","# Create a custom dataset class\n","class CustomDataset(Dataset):\n","    def __init__(self, text_data, label_data, tokenizer, max_length=256):\n","        self.text_data = text_data\n","        self.label_data = label_data\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.text_data)\n","\n","    def __getitem__(self, index):\n","      text = self.text_data[index]\n","      label = self.label_data[index]\n","\n","      # Preprocess the text using the tokenizer\n","      # Convert text to a PyTorch tensor of token ids\n","      encoded_text = self.tokenizer(\n","            text,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='pt'\n","      )\n","\n","      input_ids = encoded_text['input_ids'].squeeze(0)  # Remove the extra dimension\n","      attention_mask = encoded_text['attention_mask'].squeeze(0)\n","\n","      # Convert label to a PyTorch tensor\n","      # First, convert the label data from a string to an integer\n","      if isinstance(label, str):\n","          label = int(float(label))  # Convert label from string to integer\n","\n","      return {\n","            'input_ids': input_ids,\n","            'attention_mask': attention_mask,\n","            'labels': torch.tensor(label, dtype=torch.int64)\n","        }\n","\n","\n","# Create the datasets for training and testing\n","train_dataset = CustomDataset(text_train, label_train, tokenizer)\n","test_dataset = CustomDataset(text_test, label_test, tokenizer)\n","\n","# DataLoader parameters\n","batch_size = 32\n","shuffle = True\n","\n","# Create the DataLoader instances for training and testing\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle)\n"]},{"cell_type":"markdown","metadata":{"id":"D1ReSwbnMPaT"},"source":["## Main module"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":455,"status":"ok","timestamp":1715881931153,"user":{"displayName":"Alexis Limozin","userId":"15899913993095348392"},"user_tz":-120},"id":"b9kZ4b6PaQDA"},"outputs":[],"source":["class SMARTDeBERTaClassificationModel(nn.Module):\n","\n","    def __init__(self, model, weight = 0.02):\n","        super().__init__()\n","        self.model = model\n","        self.weight = weight\n","\n","    def forward(self, input_ids, attention_mask, labels=None):\n","        # Get initial embeddings\n","        embedder = self.model.get_input_embeddings()\n","        embed = embedder(input_ids)\n","\n","        # Define eval function\n","        def eval(embed):\n","            outputs = self.model(inputs_embeds=embed, attention_mask=attention_mask, labels=labels)\n","            return outputs.logits\n","        \n","        # Compute initial (unperturbed) state\n","        state = eval(embed)\n","        \n","        # Return here if in inference mode\n","        if labels is None:\n","            return state\n","        \n","        # Define SMART loss\n","        smart_loss_fn = SMARTLoss(eval_fn = eval, loss_fn = kl_loss, loss_last_fn = sym_kl_loss)\n","\n","        # Apply classification loss\n","        loss = F.cross_entropy(state, labels)\n","        # Apply smart loss\n","        loss += self.weight * smart_loss_fn(embed, state)\n","\n","        return state, loss"]},{"cell_type":"markdown","metadata":{"id":"4kenVzSzMVZI"},"source":["## Training"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":455},"executionInfo":{"elapsed":2779,"status":"error","timestamp":1715882050123,"user":{"displayName":"Alexis Limozin","userId":"15899913993095348392"},"user_tz":-120},"id":"O89onAjJgI2G","outputId":"648650ec-002f-4d5d-83f8-703552cfed83"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cpu\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/Users/alexislimozin/.pyenv/versions/3.10.4/envs/HateEnv/lib/python3.10/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n","  warnings.warn(\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","# Load configuration from pre-trained\n","config = AutoConfig.from_pretrained('microsoft/mdeberta-v3-base', num_labels=2)\n","base_model = DebertaV2ForSequenceClassification(config).from_pretrained('microsoft/mdeberta-v3-base')\n","model = SMARTDeBERTaClassificationModel(base_model, weight=0.02).to(device)\n","\n","if torch.cuda.device_count() > 1:\n","    print(f\"Using {torch.cuda.device_count()} GPUs\")\n","    model = nn.DataParallel(model)\n","\n","model.to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n","scaler = GradScaler()\n","accumulation_steps = 4  # Gradient accumulation steps\n"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":536,"status":"ok","timestamp":1715880790456,"user":{"displayName":"Alexis Limozin","userId":"15899913993095348392"},"user_tz":-120},"id":"dTSijC3ml58G"},"outputs":[],"source":["def train_model(model, dataloader, optimizer, device):\n","    model.train()\n","    total_loss = 0\n","    total_steps = len(dataloader)\n","\n","    for batch_idx, batch in enumerate(dataloader):\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        optimizer.zero_grad()\n","\n","        with autocast():\n","            _, loss = model(input_ids, attention_mask, labels)\n","            loss = torch.mean(loss / accumulation_steps)  # Normalize loss\n","\n","        scaler.scale(loss).backward()\n","\n","        if (batch_idx + 1) % accumulation_steps == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","\n","        total_loss += loss.item()\n","\n","        if (batch_idx + 1) % accumulation_steps == 0:\n","            print(f'Batch {batch_idx + 1}/{total_steps}, Batch Loss: {loss.item()*accumulation_steps:.4f}')\n","        \n","        del input_ids, attention_mask, labels, loss\n","        if device == \"cuda\":\n","            torch.cuda.empty_cache()\n","\n","    avg_loss = total_loss / total_steps\n","    print(f'Training Loss: {avg_loss:.4f}')\n","    return avg_loss"]},{"cell_type":"markdown","metadata":{},"source":["# Results"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1715880544739,"user":{"displayName":"Alexis Limozin","userId":"15899913993095348392"},"user_tz":-120},"id":"0iQLBjyQnVmc"},"outputs":[],"source":["def compute_metrics(pred_labels, true_labels):\n","    accuracy = accuracy_score(true_labels, pred_labels)\n","    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, pred_labels, average='binary')\n","    return {\n","        \"accuracy\": accuracy,\n","        \"precision\": precision,\n","        \"recall\": recall,\n","        \"f1\": f1\n","    }\n"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1715880544739,"user":{"displayName":"Alexis Limozin","userId":"15899913993095348392"},"user_tz":-120},"id":"_zecWOgmnjlc"},"outputs":[],"source":["def evaluate_model(model, dataloader, device):\n","    model.eval()\n","    predictions = []\n","    true_labels = []\n","    total_steps = len(dataloader)\n","    with torch.no_grad():\n","        for batch_idx, batch in enumerate(dataloader):\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","\n","            with autocast():\n","                outputs = model(input_ids, attention_mask)\n","                _, preds = torch.max(outputs, dim=1)\n","\n","            predictions.extend(preds.cpu().numpy())\n","            true_labels.extend(labels.cpu().numpy())\n","\n","    metrics = compute_metrics(predictions, true_labels)\n","    return metrics"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":474,"status":"error","timestamp":1715880795707,"user":{"displayName":"Alexis Limozin","userId":"15899913993095348392"},"user_tz":-120},"id":"ZziNZ9D7mBJu","outputId":"360a4af0-d652-4a15-dce6-f8db7f504603"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/alexislimozin/.pyenv/versions/3.10.4/envs/HateEnv/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/3\n","tensor(0.7989, grad_fn=<AddBackward0>)\n","tensor(0.7849, grad_fn=<AddBackward0>)\n","tensor(0.7813, grad_fn=<AddBackward0>)\n"]}],"source":["num_epochs = 12  # Define the number of epochs\n","\n","for epoch in range(num_epochs):\n","    print(f'Epoch {epoch+1}/{num_epochs}')\n","    train_model(model, train_loader, optimizer, device)\n","\n","    metrics = evaluate_model(model, test_loader, device)\n","    print(f'Validation Metrics: {metrics}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.save(model.state_dict(), 'smart_deberta_classification_model.pt')"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":0}
